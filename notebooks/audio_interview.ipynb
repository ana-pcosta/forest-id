{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸŒ± Pivotal Future - recruitment task\n",
    "\n",
    "This assignment is to test both your python and ML skills, and your ability to think critically about a\n",
    "problem. A set of tasks is given below. Please summarise your findings in a short report (see instructions in the last section), and attach any code you use in your analysis. \n",
    "\n",
    "Do not spend more than 8 hours on the task. Read carefully all the sections, and then complete as you many sections as you wish.\n",
    "\n",
    "You can either run the code locally on your machine, or use https://colab.google/ if you need GPU resources.\n",
    "\n",
    "### ðŸ¸ Background \n",
    "\n",
    "At Pivotal, we leverage AI for biodiversity measurement and monitoring. *Frogs* are vital indicators of environmental health due to their sensitivity to ecological changes. Monitoring their populations provides insights into ecosystem integrity and biodiversity. Accurate identification of frog species through their vocalizations is essential for effective conservation efforts. There is a high variability in frog calls. They differ based on the species (each species has a unique vocalization), type of call (mating calls, territorial calls, distress calls). This makes the task of identifying the different species challenging\n",
    "\n",
    "You can read more about frogs in the [iNaturalist page](https://www.inaturalist.org/taxa/20979-Anura). You can read more about their vocalization under the *Calls* section. \n",
    "\n",
    "### ðŸ–Šï¸ The task\n",
    "In this task, you will train and test a binary classifier to distinguish between audio recordings of frog calls and other sounds, utilizing machine learning techniques to analyze and interpret the data.\n",
    "\n",
    "The task is divided in the following parts:\n",
    "1. Load and explore the dataset\n",
    "2. Train a binary classifier model\n",
    "3. Evaluate the model\n",
    "4. Bonus part (not mandatory)\n",
    "5. Questions for report\n",
    "\n",
    "We reccomend to read every section first, and then to start coding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data exploration\n",
    "\n",
    "Download the dataset that was sent to you. This will contain two folders: \n",
    "* frogs: this folder contains audio files from 32 different frog species (variable lenght clips)\n",
    "* esc-50: this is a standard benchmark dataset for ML audio classification tasks (5s clips). 13 different classes are given. You can use this as negative class (i.e. \"no-frog\")\n",
    "\n",
    "In this section of the task, you have to: \n",
    "* inspect the dataset and make exploratory plots. Feel free to use the libraries that you prefer. \n",
    "* format the data so that you can use it for model training (i.e. folder structure, image sizes, etc...)\n",
    "\n",
    "We provided you with a utility function, read_wav_and_make_spectrogram(), to open the audio files, and convert them into visual spectrograms. You can change the size of the generated image to accomodate for the model you want to train later. Feel free to use it. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io.wavfile as wav\n",
    "from scipy.signal import spectrogram\n",
    "import os\n",
    "\n",
    "def read_wav_and_make_spectrogram(file_path, destination_path, clip_duration=3, save_clean=False, image_size=[512, 512]):\n",
    "    \"\"\"\n",
    "    Reads a WAV file, generates spectrograms for each clip of specified duration, and saves them as images.\n",
    "\n",
    "    Parameters:\n",
    "    file_path (str): Path to the WAV file.\n",
    "    destination_path (str): Path to save the spectrogram images.\n",
    "    clip_duration (int): Duration of each clip in seconds. Default is 3 seconds.\n",
    "    save_clean (bool): If True, saves spectrograms without axis labels and titles. Default is False.\n",
    "    image_size (list): Size of the output image in pixels [width, height]. Default is [512, 512].\n",
    "    \"\"\"\n",
    "    sample_rate, data = wav.read(file_path)\n",
    "    \n",
    "    if len(data.shape) > 1:\n",
    "        data = np.mean(data, axis=1).astype(np.float32)\n",
    "    \n",
    "    total_samples = len(data)\n",
    "    samples_per_clip = int(clip_duration * sample_rate)\n",
    "    num_clips = int(np.ceil(total_samples / samples_per_clip))\n",
    "    \n",
    "    if not os.path.exists(destination_path):\n",
    "        os.makedirs(destination_path)\n",
    "    \n",
    "    for i in range(num_clips):\n",
    "        start_idx = i * samples_per_clip\n",
    "        end_idx = start_idx + samples_per_clip\n",
    "        \n",
    "        segment = data[start_idx:end_idx]\n",
    "        if len(segment) < samples_per_clip:\n",
    "            segment = np.pad(segment, (0, samples_per_clip - len(segment)), mode='constant', constant_values=0)\n",
    "        \n",
    "        nperseg = 1024\n",
    "        noverlap = nperseg // 2\n",
    "        f, t, Sxx = spectrogram(segment, fs=sample_rate, nperseg=nperseg, noverlap=noverlap)\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(image_size[0] / 100, image_size[1] / 100), dpi=100)\n",
    "        ax.pcolormesh(t, f, 10 * np.log10(Sxx + 1e-10), shading='gouraud')\n",
    "        if not save_clean:\n",
    "            ax.set_ylabel('Frequency [Hz]')\n",
    "            ax.set_xlabel('Time [s]')\n",
    "            ax.set_title(f'Spectrogram - Segment {i+1}')\n",
    "            fig.colorbar(ax.pcolormesh(t, f, 10 * np.log10(Sxx + 1e-10), shading='gouraud'), label='Power (dB)')\n",
    "        else:\n",
    "            ax.set_axis_off()\n",
    "        \n",
    "        output_file = os.path.join(destination_path, f'spectrogram_{i+1}.png')\n",
    "        fig.savefig(output_file, bbox_inches='tight', pad_inches=0, dpi=100)\n",
    "        plt.close(fig)\n",
    "        \n",
    "        print(f\"Spectrogram saved as: {output_file}\")\n",
    "    \n",
    "# Example usage\n",
    "read_wav_and_make_spectrogram('datasets/audio/frogs/Ameerega_picta/Ameerega_picta_1.wav', 'output/spectrograms', save_clean=True, image_size=[512, 512])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exploratory analysis\n",
    "\n",
    "\n",
    "# data preprocessing and preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Train model\n",
    "\n",
    "In this section you have to write the code to train a binary classifier. The target classes are frog and no_frog. You are free to use any library you prefer (i.e. tensorflow, pytorch, ...). \n",
    "\n",
    "We suggest you to use a simple solution, like a simple convolutional neural network (CNN), or a pretrained model using transfer learning techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Inference and metrics\n",
    "\n",
    "Visualize the training and validation loss and accuracy to analyze the performance. Make some plots to show the performance of the model (i.e. confusion matrix, ROC curve, ...)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Bonus part (not mandatory)\n",
    "\n",
    "In this section, you can be creative and add extra parts that are not requested in the task. Possible additions could be (but are not limited to) testing effect of data augmentations or compare different model architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Write a report\n",
    "\n",
    "Please write a short and concise report (6 pages at most), or make few slides to present the results. Cover the following points:\n",
    "* Dataset description, findings and plots from the data exploration part\n",
    "* Model selected\n",
    "* Metrics of the model and error analysis\n",
    "* Bonus parts (if applicable)\n",
    "\n",
    "In your final report, please make sure to address the following questions:\n",
    "\n",
    "1. What patterns did you find during the data exploration phase?\n",
    "2. How can we address the class imbalance during training?\n",
    "3. What data augmentation techniques should be used for this particular dataset\n",
    "4. What metrics are good indicators that our model is performing well for this specific task?\n",
    "5. Did you find any patterns in the errors made by the model during the validation? \n",
    "6. In biodiverse hotspots, different species of frog (and other animals) may all call at the same time, how would you change your model to be used as a multi-label classifier to be able to predict multiple species per audio file, and what kind of data would you need?\n",
    "7. There are around 8000 species of frog, other than with a CNN, how could you use deep learning to classify them, and what kind of data would you need?\n",
    "\n",
    "\n",
    "When you are done with the code and the report, please send the update notebook and the pdf/slides at the indicated email. \n",
    "\n",
    "ðŸ€ Good luck! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
